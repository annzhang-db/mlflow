{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b479f115-ac85-4701-8fa7-cade63376b74",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Evaluate a HuggingFace LLM with mlflow.evaluate()\n",
    "\n",
    "This guide will show how to load a pre-trained HuggingFace pipeline, log it to MLflow, and use `mlflow.evaluate()` to evaluate builtin metrics as well as custom LLM-judged metrics for the model.\n",
    "\n",
    "For detailed information, please read the following documentation:\n",
    "https://mlflow.org/docs/latest/llms/llm-evaluate/index.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start MLflow Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can either:\n",
    "\n",
    "- Start a local tracking server by running `mlflow ui` within the same directory that your notebook is in\n",
    "  - Please follow [this section of the contributing guide](https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#javascript-and-ui) to get the UI set up.\n",
    "- Use a tracking server, as described in [this overview](https://mlflow.org/docs/latest/getting-started/tracking-server-overview/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "234ec3ce-4080-48da-b5ee-3f82c4a76e5f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Install necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2053bed3-8db3-498a-94e4-3f31469340ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q mlflow transformers torch torchvision evaluate datasets openai==0.27.9 tiktoken fastapi rouge_score textstat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9fc194b-839d-4494-9fc4-65a18a221c48",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Load pretrained HuggingFace pipeline\n",
    "\n",
    "Here we are loading a text summarization pipeline, but you can also use a text generation or question answering pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc4119e2-3f6c-4c28-b4a3-1b556fbeb547",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"Falconsai/text_summarization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a25d100-1d18-4cfa-a6b4-29466070651c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Log model to mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e614ce7-4765-441a-bd87-34de7aa3787c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bl/thhgmbz97z9gt3wbq1d50_b80000gp/T/ipykernel_62961/902467239.py:5: FutureWarning: The 'transformers' MLflow Models integration is known to be compatible with the following package version ranges: ``4.25.1`` -  ``4.34.1``. MLflow Models integrations with transformers may not succeed when used with package versions outside of this range.\n",
      "  model_info = mlflow.transformers.log_model(\n",
      "/Users/ann.zhang/.pyenv/versions/3.8.13/lib/python3.8/site-packages/mlflow/models/model.py:619: FutureWarning: The 'transformers' MLflow Models integration is known to be compatible with the following package version ranges: ``4.25.1`` -  ``4.34.1``. MLflow Models integrations with transformers may not succeed when used with package versions outside of this range.\n",
      "  flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\n",
      "Your max_length is set to 200, but your input_length is only 10. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=5)\n",
      "Registered model 'falconsai-summarization' already exists. Creating a new version of this model...\n",
      "2023/12/01 14:24:04 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: falconsai-summarization, version 8\n",
      "Created version '8' of model 'falconsai-summarization'.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "with mlflow.start_run():\n",
    "    model_info = mlflow.transformers.log_model(\n",
    "        transformers_model=summarizer,\n",
    "        artifact_path=\"falcons\",\n",
    "        input_example=\"Please summarize the following article:\\n article\",\n",
    "        registered_model_name=\"falconsai-summarization\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "427db365-897c-4942-8499-c0cc85f20c20",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Load Evaluation Data\n",
    "\n",
    "Load in a dataset from HuggingFace to use for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b33745ec-2171-4232-9b68-83229a5e7e4f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create our `inputs` column, we append a prompt asking to each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff7b8b9c-21ef-4bf3-9027-48938fdb5ba8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "      <th>id</th>\n",
       "      <th>inputs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(CNN)The Palestinian Authority officially beca...</td>\n",
       "      <td>Membership gives the ICC jurisdiction over all...</td>\n",
       "      <td>f001ec5c4704938247d27a44948eebb37ae98d01</td>\n",
       "      <td>Please summarize the following article:\\n(CNN)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(CNN)Never mind cats having nine lives. A stra...</td>\n",
       "      <td>Theia, a bully breed mix, was apparently hit b...</td>\n",
       "      <td>230c522854991d053fe98a718b1defa077a8efef</td>\n",
       "      <td>Please summarize the following article:\\n(CNN)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(CNN)If you've been following the news lately,...</td>\n",
       "      <td>Mohammad Javad Zarif has spent more time with ...</td>\n",
       "      <td>4495ba8f3a340d97a9df1476f8a35502bcce1f69</td>\n",
       "      <td>Please summarize the following article:\\n(CNN)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(CNN)Five Americans who were monitored for thr...</td>\n",
       "      <td>17 Americans were exposed to the Ebola virus w...</td>\n",
       "      <td>a38e72fed88684ec8d60dd5856282e999dc8c0ca</td>\n",
       "      <td>Please summarize the following article:\\n(CNN)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(CNN)A Duke student has admitted to hanging a ...</td>\n",
       "      <td>Student is no longer on Duke University campus...</td>\n",
       "      <td>c27cf1b136cc270023de959e7ab24638021bc43f</td>\n",
       "      <td>Please summarize the following article:\\n(CNN)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(CNN)He's a blue chip college basketball recru...</td>\n",
       "      <td>College-bound basketball star asks girl with D...</td>\n",
       "      <td>1b2cc634e2bfc6f2595260e7ed9b42f77ecbb0ce</td>\n",
       "      <td>Please summarize the following article:\\n(CNN)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(CNN)Governments around the world are using th...</td>\n",
       "      <td>Amnesty's annual death penalty report catalogs...</td>\n",
       "      <td>e2706dce6cf26bc61b082438188fdb6e130d9e40</td>\n",
       "      <td>Please summarize the following article:\\n(CNN)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(CNN)Andrew Getty, one of the heirs to billion...</td>\n",
       "      <td>Andrew Getty's death appears to be from natura...</td>\n",
       "      <td>0d3c8c276d079c4c225f034c69aa024cdab7869d</td>\n",
       "      <td>Please summarize the following article:\\n(CNN)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(CNN)Filipinos are being warned to be on guard...</td>\n",
       "      <td>Once a super typhoon, Maysak is now a tropical...</td>\n",
       "      <td>6222f33c2c79b80be437335eeb3f488509e92cf5</td>\n",
       "      <td>Please summarize the following article:\\n(CNN)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(CNN)For the first time in eight years, a TV l...</td>\n",
       "      <td>Bob Barker returned to host \"The Price Is Righ...</td>\n",
       "      <td>2bd8ada1de6a7b02f59430cc82045eb8d29cf033</td>\n",
       "      <td>Please summarize the following article:\\n(CNN)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  (CNN)The Palestinian Authority officially beca...   \n",
       "1  (CNN)Never mind cats having nine lives. A stra...   \n",
       "2  (CNN)If you've been following the news lately,...   \n",
       "3  (CNN)Five Americans who were monitored for thr...   \n",
       "4  (CNN)A Duke student has admitted to hanging a ...   \n",
       "5  (CNN)He's a blue chip college basketball recru...   \n",
       "6  (CNN)Governments around the world are using th...   \n",
       "7  (CNN)Andrew Getty, one of the heirs to billion...   \n",
       "8  (CNN)Filipinos are being warned to be on guard...   \n",
       "9  (CNN)For the first time in eight years, a TV l...   \n",
       "\n",
       "                                          highlights  \\\n",
       "0  Membership gives the ICC jurisdiction over all...   \n",
       "1  Theia, a bully breed mix, was apparently hit b...   \n",
       "2  Mohammad Javad Zarif has spent more time with ...   \n",
       "3  17 Americans were exposed to the Ebola virus w...   \n",
       "4  Student is no longer on Duke University campus...   \n",
       "5  College-bound basketball star asks girl with D...   \n",
       "6  Amnesty's annual death penalty report catalogs...   \n",
       "7  Andrew Getty's death appears to be from natura...   \n",
       "8  Once a super typhoon, Maysak is now a tropical...   \n",
       "9  Bob Barker returned to host \"The Price Is Righ...   \n",
       "\n",
       "                                         id  \\\n",
       "0  f001ec5c4704938247d27a44948eebb37ae98d01   \n",
       "1  230c522854991d053fe98a718b1defa077a8efef   \n",
       "2  4495ba8f3a340d97a9df1476f8a35502bcce1f69   \n",
       "3  a38e72fed88684ec8d60dd5856282e999dc8c0ca   \n",
       "4  c27cf1b136cc270023de959e7ab24638021bc43f   \n",
       "5  1b2cc634e2bfc6f2595260e7ed9b42f77ecbb0ce   \n",
       "6  e2706dce6cf26bc61b082438188fdb6e130d9e40   \n",
       "7  0d3c8c276d079c4c225f034c69aa024cdab7869d   \n",
       "8  6222f33c2c79b80be437335eeb3f488509e92cf5   \n",
       "9  2bd8ada1de6a7b02f59430cc82045eb8d29cf033   \n",
       "\n",
       "                                              inputs  \n",
       "0  Please summarize the following article:\\n(CNN)...  \n",
       "1  Please summarize the following article:\\n(CNN)...  \n",
       "2  Please summarize the following article:\\n(CNN)...  \n",
       "3  Please summarize the following article:\\n(CNN)...  \n",
       "4  Please summarize the following article:\\n(CNN)...  \n",
       "5  Please summarize the following article:\\n(CNN)...  \n",
       "6  Please summarize the following article:\\n(CNN)...  \n",
       "7  Please summarize the following article:\\n(CNN)...  \n",
       "8  Please summarize the following article:\\n(CNN)...  \n",
       "9  Please summarize the following article:\\n(CNN)...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_df = pd.DataFrame(dataset[\"test\"])\n",
    "eval_df[\"inputs\"] = \"Please summarize the following article:\\n\" + eval_df[\"article\"]\n",
    "\n",
    "display(eval_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a40f09d2-91cf-4390-a606-786cf93e6a62",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Define Extra Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom LLM-judged metric named `answer_quality` using `make_genai_metric()`. We need to define a metric definition and grading rubric, as well as some examples for the LLM judge to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d6d5fbf-883d-4eaf-8c77-93520ddb087c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluationMetric(name=answer_quality, greater_is_better=True, long_name=answer_quality, version=v1, metric_details=\n",
      "Task:\n",
      "You must return the following fields in your response one below the other:\n",
      "score: Your numerical score for the model's answer_quality based on the rubric\n",
      "justification: Your step-by-step reasoning about the model's answer_quality score\n",
      "\n",
      "You are an impartial judge. You will be given an input that was sent to a machine\n",
      "learning model, and you will be given an output that the model produced. You\n",
      "may also be given additional information that was used by the model to generate the output.\n",
      "\n",
      "Your task is to determine a numerical score called answer_quality based on the input and output.\n",
      "A definition of answer_quality and a grading rubric are provided below.\n",
      "You must use the grading rubric to determine your score. You must also justify your score.\n",
      "\n",
      "Examples could be included below for reference. Make sure to use them as references and to\n",
      "understand them before completing the task.\n",
      "\n",
      "Input:\n",
      "{input}\n",
      "\n",
      "Output:\n",
      "{output}\n",
      "\n",
      "{grading_context_columns}\n",
      "\n",
      "Metric definition:\n",
      "Please evaluate answer quality for the provided output on the following criteria: fluency, clarity, and conciseness. Each of the criteria is defined as follows:\n",
      "  - Fluency measures how naturally and smooth the output reads.\n",
      "  - Clarity measures how understandable the output is.\n",
      "  - Conciseness measures the brevity and efficiency of the output without compromising meaning.\n",
      "The more fluent, clear, and concise a text, the higher the score it deserves.\n",
      "\n",
      "\n",
      "Grading rubric:\n",
      "Answer quality: Below are the details for different scores:\n",
      "  - Score 1: The output is entirely incomprehensible and cannot be read.\n",
      "  - Score 2: The output conveys some meaning, but needs lots of improvement in to improve fluency, clarity, and conciseness.\n",
      "  - Score 3: The output is understandable but still needs improvement.\n",
      "  - Score 4: The output performs well on two of fluency, clarity, and conciseness, but could be improved on one of these criteria.\n",
      "  - Score 5: The output reads smoothly, is easy to understand, and clear. There is no clear way to improve the output on these criteria.\n",
      "\n",
      "Examples:\n",
      "\n",
      "Example Input:\n",
      "What is MLflow?\n",
      "\n",
      "Example Output:\n",
      "MLflow is an open-source platform. For managing machine learning workflows, it including experiment tracking model packaging versioning and deployment as well as a platform simplifying for on the ML lifecycle.\n",
      "\n",
      "\n",
      "\n",
      "Example score: 2\n",
      "Example justification: The output is difficult to understand and demonstrates extremely low clarity. However, it still conveys some meaning so this output deserves a score of 2.\n",
      "        \n",
      "\n",
      "Example Input:\n",
      "What is MLflow?\n",
      "\n",
      "Example Output:\n",
      "MLflow is an open-source platform for managing machine learning workflows, including experiment tracking, model packaging, versioning, and deployment.\n",
      "\n",
      "\n",
      "\n",
      "Example score: 5\n",
      "Example justification: The output is easily understandable, clear, and concise. It deserves a score of 5.\n",
      "        \n",
      "\n",
      "You must return the following fields in your response one below the other:\n",
      "score: Your numerical score for the model's answer_quality based on the rubric\n",
      "justification: Your step-by-step reasoning about the model's answer_quality score\n",
      "    )\n"
     ]
    }
   ],
   "source": [
    "from mlflow.metrics.genai import EvaluationExample, make_genai_metric\n",
    "\n",
    "answer_quality_definition = \"\"\"Please evaluate answer quality for the provided output on the following criteria: fluency, clarity, and conciseness. Each of the criteria is defined as follows:\n",
    "  - Fluency measures how naturally and smooth the output reads.\n",
    "  - Clarity measures how understandable the output is.\n",
    "  - Conciseness measures the brevity and efficiency of the output without compromising meaning.\n",
    "The more fluent, clear, and concise a text, the higher the score it deserves.\n",
    "\"\"\"\n",
    "\n",
    "answer_quality_rubric = \"\"\"Answer quality: Below are the details for different scores:\n",
    "  - Score 1: The output is entirely incomprehensible and cannot be read.\n",
    "  - Score 2: The output conveys some meaning, but needs lots of improvement in to improve fluency, clarity, and conciseness.\n",
    "  - Score 3: The output is understandable but still needs improvement.\n",
    "  - Score 4: The output performs well on two of fluency, clarity, and conciseness, but could be improved on one of these criteria.\n",
    "  - Score 5: The output reads smoothly, is easy to understand, and clear. There is no clear way to improve the output on these criteria.\"\"\"\n",
    "\n",
    "example1 = EvaluationExample(\n",
    "    input=\"What is MLflow?\",\n",
    "    output=\"MLflow is an open-source platform. For managing machine learning workflows, it including experiment tracking model packaging versioning and deployment as well as a platform simplifying for on the ML lifecycle.\",\n",
    "    score=2,\n",
    "    justification=\"The output is difficult to understand and demonstrates extremely low clarity. However, it still conveys some meaning so this output deserves a score of 2.\",\n",
    ")\n",
    "\n",
    "example2 = EvaluationExample(\n",
    "    input=\"What is MLflow?\",\n",
    "    output=\"MLflow is an open-source platform for managing machine learning workflows, including experiment tracking, model packaging, versioning, and deployment.\",\n",
    "    score=5,\n",
    "    justification=\"The output is easily understandable, clear, and concise. It deserves a score of 5.\",\n",
    ")\n",
    "\n",
    "answer_quality_metric = make_genai_metric(\n",
    "    name=\"answer_quality\",\n",
    "    definition=answer_quality_definition,\n",
    "    grading_prompt=answer_quality_rubric,\n",
    "    version=\"v1\",\n",
    "    examples=[example1, example2],\n",
    "    model=\"openai:/gpt-4\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "print(answer_quality_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also load one of the predefined metrics - in this case we are using answer_correctness with GPT-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6321722b-cfec-49dd-b99e-84ac2370e84d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluationMetric(name=answer_correctness, greater_is_better=True, long_name=answer_correctness, version=v1, metric_details=\n",
      "Task:\n",
      "You must return the following fields in your response one below the other:\n",
      "score: Your numerical score for the model's answer_correctness based on the rubric\n",
      "justification: Your step-by-step reasoning about the model's answer_correctness score\n",
      "\n",
      "You are an impartial judge. You will be given an input that was sent to a machine\n",
      "learning model, and you will be given an output that the model produced. You\n",
      "may also be given additional information that was used by the model to generate the output.\n",
      "\n",
      "Your task is to determine a numerical score called answer_correctness based on the input and output.\n",
      "A definition of answer_correctness and a grading rubric are provided below.\n",
      "You must use the grading rubric to determine your score. You must also justify your score.\n",
      "\n",
      "Examples could be included below for reference. Make sure to use them as references and to\n",
      "understand them before completing the task.\n",
      "\n",
      "Input:\n",
      "{input}\n",
      "\n",
      "Output:\n",
      "{output}\n",
      "\n",
      "{grading_context_columns}\n",
      "\n",
      "Metric definition:\n",
      "Answer correctness is evaluated on the accuracy of the provided output based on the provided targets, which is the ground truth. Scores can be assigned based on the degree of semantic similarity and factual correctness of the provided output to the provided targets, where a higher score indicates higher degree of accuracy.\n",
      "\n",
      "Grading rubric:\n",
      "Answer Correctness: Below are the details for different scores:\n",
      "- Score 1: The output is completely incorrect. It is completely different from or contradicts the provided targets.\n",
      "- Score 2: The output demonstrates some degree of semantic similarity and includes partially correct information. However, the output still has significant discrepancies with the provided targets or inaccuracies.\n",
      "- Score 3: The output addresses a couple of aspects of the input accurately, aligning with the provided targets. However, there are still omissions or minor inaccuracies.\n",
      "- Score 4: The output is mostly correct. It provides mostly accurate information, but there may be one or more minor omissions or inaccuracies.\n",
      "- Score 5: The output is correct. It demonstrates a high degree of accuracy and semantic similarity to the targets.\n",
      "\n",
      "Examples:\n",
      "\n",
      "Example Input:\n",
      "How is MLflow related to Databricks?\n",
      "\n",
      "Example Output:\n",
      "Databricks is a data engineering and analytics platform designed to help organizations process and analyze large amounts of data. Databricks is a company specializing in big data and machine learning solutions.\n",
      "\n",
      "Additional information used by the model:\n",
      "key: targets\n",
      "value:\n",
      "MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is designed to address the challenges that data scientists and machine learning engineers face when developing, training, and deploying machine learning models.\n",
      "\n",
      "Example score: 2\n",
      "Example justification: The output provided by the model does demonstrate some degree of semantic similarity to the targets, as it correctly identifies Databricks as a company specializing in big data and machine learning solutions. However, it fails to address the main point of the input question, which is the relationship between MLflow and Databricks. The output does not mention MLflow at all, which is a significant discrepancy with the provided targets. Therefore, the model's answer_correctness score is 2.\n",
      "        \n",
      "\n",
      "Example Input:\n",
      "How is MLflow related to Databricks?\n",
      "\n",
      "Example Output:\n",
      "MLflow is a product created by Databricks to enhance the efficiency of machine learning processes.\n",
      "\n",
      "Additional information used by the model:\n",
      "key: targets\n",
      "value:\n",
      "MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is designed to address the challenges that data scientists and machine learning engineers face when developing, training, and deploying machine learning models.\n",
      "\n",
      "Example score: 4\n",
      "Example justification: The output provided by the model is mostly correct. It correctly identifies that MLflow is a product created by Databricks. However, it does not mention that MLflow is an open-source platform for managing the end-to-end machine learning lifecycle, which is a significant part of its function. Therefore, while the output is mostly accurate, it has a minor omission, which is why it gets a score of 4 according to the grading rubric.\n",
      "        \n",
      "\n",
      "You must return the following fields in your response one below the other:\n",
      "score: Your numerical score for the model's answer_correctness based on the rubric\n",
      "justification: Your step-by-step reasoning about the model's answer_correctness score\n",
      "    )\n"
     ]
    }
   ],
   "source": [
    "from mlflow.metrics.genai import answer_correctness\n",
    "\n",
    "answer_correctness_metric = answer_correctness(model=\"openai:/gpt-4\")\n",
    "\n",
    "print(answer_correctness_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac9cdf24-d024-4543-a292-371bb301041f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to set our OpenAI API key, since we are using GPT-4 for our LLM-judged metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b56fb6b-2272-4ddf-b297-b222a13efb77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"redacted\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `mlflow.evaluate()` on the first 10 rows of the data. Using the 'text-summarization' model, we get toxicity, readability metrics, and rouge score as builtin metrics. We also pass in the two metrics we defined above into the extra_metrics parameter to be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da8e792d-7750-40c2-b55f-4f902e6d9442",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts:   0%|          | 0/13 [00:00<?, ?it/s]2023/12/01 14:24:05 INFO mlflow.store.artifact.artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false\n",
      "Downloading artifacts: 100%|██████████| 13/13 [00:10<00:00,  1.29it/s] \n",
      "2023/12/01 14:24:15 WARNING mlflow.transformers: Could not specify device parameter for this pipeline type\n",
      "2023/12/01 14:24:16 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/12/01 14:24:16 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (789 > 512). Running this sequence through the model will result in indexing errors\n",
      "Your max_length is set to 200, but your input_length is only 161. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=80)\n",
      "2023/12/01 14:24:40 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "100%|██████████| 1/1 [00:07<00:00,  7.50s/it]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.08s/it]\n",
      "2023/12/01 14:24:51 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: token_count\n",
      "2023/12/01 14:24:51 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: toxicity\n",
      "2023/12/01 14:24:52 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: flesch_kincaid_grade_level\n",
      "2023/12/01 14:24:52 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: ari_grade_level\n",
      "2023/12/01 14:24:52 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: rouge1\n",
      "2023/12/01 14:24:52 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: rouge2\n",
      "2023/12/01 14:24:52 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: rougeL\n",
      "2023/12/01 14:24:52 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: rougeLsum\n",
      "2023/12/01 14:24:52 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: answer_correctness\n",
      "100%|██████████| 10/10 [00:13<00:00,  1.34s/it]\n",
      "2023/12/01 14:25:05 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: answer_quality\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.23it/s]\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "with mlflow.start_run():\n",
    "    results = mlflow.evaluate(\n",
    "        model_info.model_uri,\n",
    "        eval_df.head(10),\n",
    "        model_type=\"text-summarization\",\n",
    "        targets=\"highlights\",\n",
    "        extra_metrics=[answer_correctness_metric, answer_quality_metric],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`results.metrics` is a dictionary with the aggregate values for all the metrics calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "831509bc-69b2-4513-9d9a-a65af71e8e0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toxicity/v1/mean': 0.0016064770505181513,\n",
       " 'toxicity/v1/variance': 1.9989291306231105e-06,\n",
       " 'toxicity/v1/p90': 0.0036141288001090284,\n",
       " 'toxicity/v1/ratio': 0.0,\n",
       " 'flesch_kincaid_grade_level/v1/mean': 7.31,\n",
       " 'flesch_kincaid_grade_level/v1/variance': 4.450899999999999,\n",
       " 'flesch_kincaid_grade_level/v1/p90': 9.79,\n",
       " 'ari_grade_level/v1/mean': 8.84,\n",
       " 'ari_grade_level/v1/variance': 3.9204000000000008,\n",
       " 'ari_grade_level/v1/p90': 11.3,\n",
       " 'rouge1/v1/mean': 0.3388889861705919,\n",
       " 'rouge1/v1/variance': 0.010298884566027731,\n",
       " 'rouge1/v1/p90': 0.4338874680306905,\n",
       " 'rouge2/v1/mean': 0.12654092075859483,\n",
       " 'rouge2/v1/variance': 0.003022103718907744,\n",
       " 'rouge2/v1/p90': 0.18981132075471696,\n",
       " 'rougeL/v1/mean': 0.23649862112921,\n",
       " 'rougeL/v1/variance': 0.004164158957865274,\n",
       " 'rougeL/v1/p90': 0.28343873517786566,\n",
       " 'rougeLsum/v1/mean': 0.2624236382358652,\n",
       " 'rougeLsum/v1/variance': 0.006182113389247971,\n",
       " 'rougeLsum/v1/p90': 0.3278787878787879,\n",
       " 'answer_correctness/v1/mean': 3.4444444444444446,\n",
       " 'answer_correctness/v1/variance': 0.24691358024691354,\n",
       " 'answer_correctness/v1/p90': 4.0,\n",
       " 'answer_quality/v1/mean': 3.6666666666666665,\n",
       " 'answer_quality/v1/variance': 0.2222222222222222,\n",
       " 'answer_quality/v1/p90': 4.0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view `eval_results_table`, which shows us the metrics for each row of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71dc600a-ea96-40a4-a643-efb7580a741c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 95.26it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>id</th>\n",
       "      <th>inputs</th>\n",
       "      <th>highlights</th>\n",
       "      <th>outputs</th>\n",
       "      <th>token_count</th>\n",
       "      <th>toxicity/v1/score</th>\n",
       "      <th>flesch_kincaid_grade_level/v1/score</th>\n",
       "      <th>ari_grade_level/v1/score</th>\n",
       "      <th>rouge1/v1/score</th>\n",
       "      <th>rouge2/v1/score</th>\n",
       "      <th>rougeL/v1/score</th>\n",
       "      <th>rougeLsum/v1/score</th>\n",
       "      <th>answer_correctness/v1/score</th>\n",
       "      <th>answer_correctness/v1/justification</th>\n",
       "      <th>answer_quality/v1/score</th>\n",
       "      <th>answer_quality/v1/justification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(CNN)The Palestinian Authority officially beca...</td>\n",
       "      <td>f001ec5c4704938247d27a44948eebb37ae98d01</td>\n",
       "      <td>Please summarize the following article:\\n(CNN)...</td>\n",
       "      <td>Membership gives the ICC jurisdiction over all...</td>\n",
       "      <td>The Palestinian Authority officially became th...</td>\n",
       "      <td>47</td>\n",
       "      <td>0.001351</td>\n",
       "      <td>9.7</td>\n",
       "      <td>11.3</td>\n",
       "      <td>0.324324</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.270270</td>\n",
       "      <td>0.270270</td>\n",
       "      <td>4.0</td>\n",
       "      <td>The output provided by the model is mostly cor...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>The output is understandable and fluent, but i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(CNN)Never mind cats having nine lives. A stra...</td>\n",
       "      <td>230c522854991d053fe98a718b1defa077a8efef</td>\n",
       "      <td>Please summarize the following article:\\n(CNN)...</td>\n",
       "      <td>Theia, a bully breed mix, was apparently hit b...</td>\n",
       "      <td>Theia is a friendly white-and-black bully bree...</td>\n",
       "      <td>54</td>\n",
       "      <td>0.002589</td>\n",
       "      <td>5.7</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.456522</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>3.0</td>\n",
       "      <td>The output provided by the model addresses a c...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>The output is fluent and clear, as it reads sm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(CNN)If you've been following the news lately,...</td>\n",
       "      <td>4495ba8f3a340d97a9df1476f8a35502bcce1f69</td>\n",
       "      <td>Please summarize the following article:\\n(CNN)...</td>\n",
       "      <td>Mohammad Javad Zarif has spent more time with ...</td>\n",
       "      <td>Mohammad Javad Zarif has been U.S. Secretary o...</td>\n",
       "      <td>58</td>\n",
       "      <td>0.004271</td>\n",
       "      <td>8.1</td>\n",
       "      <td>9.3</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.136986</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>3.0</td>\n",
       "      <td>The output provided by the model addresses a c...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>The output is understandable but still needs i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(CNN)Five Americans who were monitored for thr...</td>\n",
       "      <td>a38e72fed88684ec8d60dd5856282e999dc8c0ca</td>\n",
       "      <td>Please summarize the following article:\\n(CNN)...</td>\n",
       "      <td>17 Americans were exposed to the Ebola virus w...</td>\n",
       "      <td>Five americans who were monitored for three we...</td>\n",
       "      <td>53</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>6.9</td>\n",
       "      <td>9.7</td>\n",
       "      <td>0.382022</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.247191</td>\n",
       "      <td>0.314607</td>\n",
       "      <td>4.0</td>\n",
       "      <td>The output provided by the model is mostly cor...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>The output is fluent and clear, as it provides...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(CNN)A Duke student has admitted to hanging a ...</td>\n",
       "      <td>c27cf1b136cc270023de959e7ab24638021bc43f</td>\n",
       "      <td>Please summarize the following article:\\n(CNN)...</td>\n",
       "      <td>Student is no longer on Duke University campus...</td>\n",
       "      <td>Duke student admits to hanging a noose from a ...</td>\n",
       "      <td>44</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>5.9</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Failed to extract score and justification. Raw...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Failed to extract score and justification. Raw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(CNN)He's a blue chip college basketball recru...</td>\n",
       "      <td>1b2cc634e2bfc6f2595260e7ed9b42f77ecbb0ce</td>\n",
       "      <td>Please summarize the following article:\\n(CNN)...</td>\n",
       "      <td>College-bound basketball star asks girl with D...</td>\n",
       "      <td>Trey Moses and Ellie Meredith asked Ellie to b...</td>\n",
       "      <td>50</td>\n",
       "      <td>0.003541</td>\n",
       "      <td>3.5</td>\n",
       "      <td>6.3</td>\n",
       "      <td>0.061538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061538</td>\n",
       "      <td>0.061538</td>\n",
       "      <td>3.0</td>\n",
       "      <td>The output provided by the model addresses a c...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>The output is understandable but still needs i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(CNN)Governments around the world are using th...</td>\n",
       "      <td>e2706dce6cf26bc61b082438188fdb6e130d9e40</td>\n",
       "      <td>Please summarize the following article:\\n(CNN)...</td>\n",
       "      <td>Amnesty's annual death penalty report catalogs...</td>\n",
       "      <td>Amnesty International says governments are usi...</td>\n",
       "      <td>63</td>\n",
       "      <td>0.002296</td>\n",
       "      <td>10.6</td>\n",
       "      <td>11.3</td>\n",
       "      <td>0.431373</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.274510</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>The output provided by the model addresses a c...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>The output is fluent and clear, as it reads sm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(CNN)Andrew Getty, one of the heirs to billion...</td>\n",
       "      <td>0d3c8c276d079c4c225f034c69aa024cdab7869d</td>\n",
       "      <td>Please summarize the following article:\\n(CNN)...</td>\n",
       "      <td>Andrew Getty's death appears to be from natura...</td>\n",
       "      <td>The coroner's preliminary assessment is there ...</td>\n",
       "      <td>48</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>9.3</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.361446</td>\n",
       "      <td>0.098765</td>\n",
       "      <td>0.192771</td>\n",
       "      <td>0.192771</td>\n",
       "      <td>4.0</td>\n",
       "      <td>The output provided by the model is mostly cor...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>The output is fluent and clear, as it reads sm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(CNN)Filipinos are being warned to be on guard...</td>\n",
       "      <td>6222f33c2c79b80be437335eeb3f488509e92cf5</td>\n",
       "      <td>Please summarize the following article:\\n(CNN)...</td>\n",
       "      <td>Once a super typhoon, Maysak is now a tropical...</td>\n",
       "      <td>Maysak is now classified as a tropical storm, ...</td>\n",
       "      <td>59</td>\n",
       "      <td>0.000935</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0.338028</td>\n",
       "      <td>0.144928</td>\n",
       "      <td>0.253521</td>\n",
       "      <td>0.281690</td>\n",
       "      <td>4.0</td>\n",
       "      <td>The output provided by the model is mostly cor...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>The output is fluent and clear, as it provides...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(CNN)For the first time in eight years, a TV l...</td>\n",
       "      <td>2bd8ada1de6a7b02f59430cc82045eb8d29cf033</td>\n",
       "      <td>Please summarize the following article:\\n(CNN)...</td>\n",
       "      <td>Bob Barker returned to host \"The Price Is Righ...</td>\n",
       "      <td>Bob Barker hosted \"The Price Is Right\" for 35 ...</td>\n",
       "      <td>44</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>5.4</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.188679</td>\n",
       "      <td>0.290909</td>\n",
       "      <td>0.327273</td>\n",
       "      <td>3.0</td>\n",
       "      <td>The output provided by the model addresses a c...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>The output is fluent and clear, as it is easy ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  (CNN)The Palestinian Authority officially beca...   \n",
       "1  (CNN)Never mind cats having nine lives. A stra...   \n",
       "2  (CNN)If you've been following the news lately,...   \n",
       "3  (CNN)Five Americans who were monitored for thr...   \n",
       "4  (CNN)A Duke student has admitted to hanging a ...   \n",
       "5  (CNN)He's a blue chip college basketball recru...   \n",
       "6  (CNN)Governments around the world are using th...   \n",
       "7  (CNN)Andrew Getty, one of the heirs to billion...   \n",
       "8  (CNN)Filipinos are being warned to be on guard...   \n",
       "9  (CNN)For the first time in eight years, a TV l...   \n",
       "\n",
       "                                         id  \\\n",
       "0  f001ec5c4704938247d27a44948eebb37ae98d01   \n",
       "1  230c522854991d053fe98a718b1defa077a8efef   \n",
       "2  4495ba8f3a340d97a9df1476f8a35502bcce1f69   \n",
       "3  a38e72fed88684ec8d60dd5856282e999dc8c0ca   \n",
       "4  c27cf1b136cc270023de959e7ab24638021bc43f   \n",
       "5  1b2cc634e2bfc6f2595260e7ed9b42f77ecbb0ce   \n",
       "6  e2706dce6cf26bc61b082438188fdb6e130d9e40   \n",
       "7  0d3c8c276d079c4c225f034c69aa024cdab7869d   \n",
       "8  6222f33c2c79b80be437335eeb3f488509e92cf5   \n",
       "9  2bd8ada1de6a7b02f59430cc82045eb8d29cf033   \n",
       "\n",
       "                                              inputs  \\\n",
       "0  Please summarize the following article:\\n(CNN)...   \n",
       "1  Please summarize the following article:\\n(CNN)...   \n",
       "2  Please summarize the following article:\\n(CNN)...   \n",
       "3  Please summarize the following article:\\n(CNN)...   \n",
       "4  Please summarize the following article:\\n(CNN)...   \n",
       "5  Please summarize the following article:\\n(CNN)...   \n",
       "6  Please summarize the following article:\\n(CNN)...   \n",
       "7  Please summarize the following article:\\n(CNN)...   \n",
       "8  Please summarize the following article:\\n(CNN)...   \n",
       "9  Please summarize the following article:\\n(CNN)...   \n",
       "\n",
       "                                          highlights  \\\n",
       "0  Membership gives the ICC jurisdiction over all...   \n",
       "1  Theia, a bully breed mix, was apparently hit b...   \n",
       "2  Mohammad Javad Zarif has spent more time with ...   \n",
       "3  17 Americans were exposed to the Ebola virus w...   \n",
       "4  Student is no longer on Duke University campus...   \n",
       "5  College-bound basketball star asks girl with D...   \n",
       "6  Amnesty's annual death penalty report catalogs...   \n",
       "7  Andrew Getty's death appears to be from natura...   \n",
       "8  Once a super typhoon, Maysak is now a tropical...   \n",
       "9  Bob Barker returned to host \"The Price Is Righ...   \n",
       "\n",
       "                                             outputs  token_count  \\\n",
       "0  The Palestinian Authority officially became th...           47   \n",
       "1  Theia is a friendly white-and-black bully bree...           54   \n",
       "2  Mohammad Javad Zarif has been U.S. Secretary o...           58   \n",
       "3  Five americans who were monitored for three we...           53   \n",
       "4  Duke student admits to hanging a noose from a ...           44   \n",
       "5  Trey Moses and Ellie Meredith asked Ellie to b...           50   \n",
       "6  Amnesty International says governments are usi...           63   \n",
       "7  The coroner's preliminary assessment is there ...           48   \n",
       "8  Maysak is now classified as a tropical storm, ...           59   \n",
       "9  Bob Barker hosted \"The Price Is Right\" for 35 ...           44   \n",
       "\n",
       "   toxicity/v1/score  flesch_kincaid_grade_level/v1/score  \\\n",
       "0           0.001351                                  9.7   \n",
       "1           0.002589                                  5.7   \n",
       "2           0.004271                                  8.1   \n",
       "3           0.000439                                  6.9   \n",
       "4           0.000296                                  5.9   \n",
       "5           0.003541                                  3.5   \n",
       "6           0.002296                                 10.6   \n",
       "7           0.000145                                  9.3   \n",
       "8           0.000935                                  8.0   \n",
       "9           0.000201                                  5.4   \n",
       "\n",
       "   ari_grade_level/v1/score  rouge1/v1/score  rouge2/v1/score  \\\n",
       "0                      11.3         0.324324         0.166667   \n",
       "1                       6.7         0.456522         0.088889   \n",
       "2                       9.3         0.320000         0.136986   \n",
       "3                       9.7         0.382022         0.137931   \n",
       "4                       6.0         0.350000         0.102564   \n",
       "5                       6.3         0.061538         0.000000   \n",
       "6                      11.3         0.431373         0.200000   \n",
       "7                      11.0         0.361446         0.098765   \n",
       "8                       9.4         0.338028         0.144928   \n",
       "9                       7.4         0.363636         0.188679   \n",
       "\n",
       "   rougeL/v1/score  rougeLsum/v1/score  answer_correctness/v1/score  \\\n",
       "0         0.270270            0.270270                          4.0   \n",
       "1         0.282609            0.326087                          3.0   \n",
       "2         0.266667            0.266667                          3.0   \n",
       "3         0.247191            0.314607                          4.0   \n",
       "4         0.225000            0.250000                          NaN   \n",
       "5         0.061538            0.061538                          3.0   \n",
       "6         0.274510            0.333333                          3.0   \n",
       "7         0.192771            0.192771                          4.0   \n",
       "8         0.253521            0.281690                          4.0   \n",
       "9         0.290909            0.327273                          3.0   \n",
       "\n",
       "                 answer_correctness/v1/justification  answer_quality/v1/score  \\\n",
       "0  The output provided by the model is mostly cor...                      3.0   \n",
       "1  The output provided by the model addresses a c...                      4.0   \n",
       "2  The output provided by the model addresses a c...                      3.0   \n",
       "3  The output provided by the model is mostly cor...                      4.0   \n",
       "4  Failed to extract score and justification. Raw...                      NaN   \n",
       "5  The output provided by the model addresses a c...                      3.0   \n",
       "6  The output provided by the model addresses a c...                      4.0   \n",
       "7  The output provided by the model is mostly cor...                      4.0   \n",
       "8  The output provided by the model is mostly cor...                      4.0   \n",
       "9  The output provided by the model addresses a c...                      4.0   \n",
       "\n",
       "                     answer_quality/v1/justification  \n",
       "0  The output is understandable and fluent, but i...  \n",
       "1  The output is fluent and clear, as it reads sm...  \n",
       "2  The output is understandable but still needs i...  \n",
       "3  The output is fluent and clear, as it provides...  \n",
       "4  Failed to extract score and justification. Raw...  \n",
       "5  The output is understandable but still needs i...  \n",
       "6  The output is fluent and clear, as it reads sm...  \n",
       "7  The output is fluent and clear, as it reads sm...  \n",
       "8  The output is fluent and clear, as it provides...  \n",
       "9  The output is fluent and clear, as it is easy ...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.tables[\"eval_results_table\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can view our evaluation results in the MLflow UI under the Evaluation tab. Here, we can choose which columns to group by and a column to compare on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/uDmh4M0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Evaluate HuggingFace LLM with mlflow.evaluate()",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "mlflow-dev-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
