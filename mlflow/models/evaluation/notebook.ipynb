{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/01/31 11:33:03 WARNING mlflow.pyfunc: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - mlflow (current: 2.6.1.dev0, required: mlflow==2.9.2)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n",
      "2024/01/31 11:33:04 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2024/01/31 11:33:04 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n",
      "2024/01/31 11:33:06 INFO mlflow.models.evaluation.default_evaluator: column name predictions\n",
      "2024/01/31 11:33:06 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:06 INFO mlflow.models.evaluation.default_evaluator: column name metrics\n",
      "2024/01/31 11:33:06 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:06 INFO mlflow.models.evaluation.default_evaluator: column name inputs\n",
      "2024/01/31 11:33:06 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:06 INFO mlflow.models.evaluation.default_evaluator: column name targets\n",
      "2024/01/31 11:33:06 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:06 INFO mlflow.models.evaluation.default_evaluator: column name targets\n",
      "2024/01/31 11:33:06 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:06 INFO mlflow.models.evaluation.default_evaluator: column name predictions\n",
      "2024/01/31 11:33:06 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:06 INFO mlflow.models.evaluation.default_evaluator: column name answer_similarity\n",
      "2024/01/31 11:33:06 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:06 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "2024/01/31 11:33:06 INFO mlflow.models.evaluation.default_evaluator: column name predictions\n",
      "2024/01/31 11:33:06 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:06 INFO mlflow.models.evaluation.default_evaluator: column name targets\n",
      "2024/01/31 11:33:06 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:06 INFO mlflow.models.evaluation.default_evaluator: column name metrics\n",
      "2024/01/31 11:33:06 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:06 INFO mlflow.models.evaluation.default_evaluator: column name predictions\n",
      "2024/01/31 11:33:06 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:06 INFO mlflow.models.evaluation.default_evaluator: column name targets\n",
      "2024/01/31 11:33:06 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:06 INFO mlflow.models.evaluation.default_evaluator: column name metrics\n",
      "2024/01/31 11:33:06 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "Using default facebook/roberta-hate-speech-dynabench-r4-target checkpoint\n",
      "2024/01/31 11:33:14 INFO mlflow.models.evaluation.default_evaluator: column name predictions\n",
      "2024/01/31 11:33:14 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:14 INFO mlflow.models.evaluation.default_evaluator: column name targets\n",
      "2024/01/31 11:33:14 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:14 INFO mlflow.models.evaluation.default_evaluator: column name metrics\n",
      "2024/01/31 11:33:14 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:14 INFO mlflow.models.evaluation.default_evaluator: column name predictions\n",
      "2024/01/31 11:33:14 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:14 INFO mlflow.models.evaluation.default_evaluator: column name targets\n",
      "2024/01/31 11:33:14 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:14 INFO mlflow.models.evaluation.default_evaluator: column name metrics\n",
      "2024/01/31 11:33:14 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:14 INFO mlflow.models.evaluation.default_evaluator: column name predictions\n",
      "2024/01/31 11:33:14 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:14 INFO mlflow.models.evaluation.default_evaluator: column name targets\n",
      "2024/01/31 11:33:14 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:14 INFO mlflow.models.evaluation.default_evaluator: column name metrics\n",
      "2024/01/31 11:33:14 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:14 INFO mlflow.models.evaluation.default_evaluator: column name sample_weight\n",
      "2024/01/31 11:33:14 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:14 INFO mlflow.models.evaluation.default_evaluator: column name predictions\n",
      "2024/01/31 11:33:14 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:14 INFO mlflow.models.evaluation.default_evaluator: column name metrics\n",
      "2024/01/31 11:33:14 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:14 INFO mlflow.models.evaluation.default_evaluator: column name inputs\n",
      "2024/01/31 11:33:14 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:14 INFO mlflow.models.evaluation.default_evaluator: column name targets\n",
      "2024/01/31 11:33:14 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab8adef792834bd5981024ea2517cb51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: column name targets\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: column name predictions\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: column name answer_similarity\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: column name predictions\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: column name targets\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: column name metrics\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: column name predictions\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: column name targets\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: column name metrics\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer sim MetricValue(scores=[4, None, None], justifications=[\"The model's output accurately describes MLflow as an open-source platform for managing the machine learning lifecycle, including deployment, tracking, and reproducibility of models. However, it does not mention that it was developed by Databricks or the challenges it addresses for data scientists and machine learning engineers, which are included in the target information.\", None, None], aggregate_results={'mean': 4.0, 'variance': 0.0, 'p90': 4.0})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: column name predictions\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: column name targets\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: column name metrics\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: column name predictions\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: column name targets\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: column name metrics\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: column name predictions\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: column name targets\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: column name metrics\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: column name sample_weight\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: column name predictions\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: column name metrics\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: column name inputs\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: column name targets\n",
      "2024/01/31 11:33:19 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0df3a1a445742d7bc15329a7a133973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/01/31 11:33:26 INFO mlflow.models.evaluation.default_evaluator: column name targets\n",
      "2024/01/31 11:33:26 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:26 INFO mlflow.models.evaluation.default_evaluator: column name predictions\n",
      "2024/01/31 11:33:26 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n",
      "2024/01/31 11:33:26 INFO mlflow.models.evaluation.default_evaluator: column name answer_similarity\n",
      "2024/01/31 11:33:26 INFO mlflow.models.evaluation.default_evaluator: input df columns Index(['inputs'], dtype='object')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer sim MetricValue(scores=[4, 4, 4], justifications=['The output effectively explains what MLflow is and its purpose, aligning with the provided targets in most aspects. However, it does not mention that MLflow was developed by Databricks, which is included in the target information.', \"The model's output accurately describes Apache Spark as an open-source distributed computing system used for big data processing and analytics, which aligns well with the provided targets. However, it lacks details about Spark's components and its development history, which prevents it from achieving a perfect score.\", 'The output effectively explains what Python is, its uses, and its features, aligning closely with the provided targets. However, it does not mention the creator and the year of release, which are included in the targets, hence it does not fully align in all significant aspects.'], aggregate_results={'mean': 4.0, 'variance': 0.0, 'p90': 4.0})\n",
      "<mlflow.models.evaluation.base.EvaluationResult object at 0x2a693a790>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108528568d2d455aa08233cb0400b9a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            inputs                                       ground_truth  \\\n",
      "0  What is MLflow?  MLflow is an open-source platform for managing...   \n",
      "1   What is Spark?  Apache Spark is an open-source, distributed co...   \n",
      "2  What is Python?  Python is a high-level programming language th...   \n",
      "\n",
      "                                             outputs  token_count  \\\n",
      "0  MLflow is an open-source platform for the comp...           48   \n",
      "1  Spark is an open-source distributed computing ...           35   \n",
      "2  Python is a high-level, interpreted programmin...           53   \n",
      "\n",
      "   toxicity/v1/score  flesch_kincaid_grade_level/v1/score  \\\n",
      "0           0.000138                                 13.1   \n",
      "1           0.000139                                 12.9   \n",
      "2           0.000139                                 16.8   \n",
      "\n",
      "   ari_grade_level/v1/score  answer_similarity/v1/score  \\\n",
      "0                      17.6                           4   \n",
      "1                      14.1                           4   \n",
      "2                      19.4                           4   \n",
      "\n",
      "                  answer_similarity/v1/justification  \n",
      "0  The output effectively explains what MLflow is...  \n",
      "1  The model's output accurately describes Apache...  \n",
      "2  The output effectively explains what Python is...  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "import mlflow\n",
    "from mlflow.metrics import make_metric\n",
    "from mlflow.metrics.genai import EvaluationExample, answer_similarity\n",
    "\n",
    "assert \"OPENAI_API_KEY\" in os.environ, \"Please set the OPENAI_API_KEY environment variable.\"\n",
    "\n",
    "\n",
    "def custom_metric(targets, predictions, answer_similarity):\n",
    "    print(\"answer sim\", answer_similarity)\n",
    "    return 8  # Some dummy value\n",
    "\n",
    "\n",
    "# testing with OpenAI gpt-3.5-turbo\n",
    "example = EvaluationExample(\n",
    "    input=\"What is MLflow?\",\n",
    "    output=\"MLflow is an open-source platform for managing machine \"\n",
    "    \"learning workflows, including experiment tracking, model packaging, \"\n",
    "    \"versioning, and deployment, simplifying the ML lifecycle.\",\n",
    "    score=4,\n",
    "    justification=\"The definition effectively explains what MLflow is \"\n",
    "    \"its purpose, and its developer. It could be more concise for a 5-score.\",\n",
    "    grading_context={\n",
    "        \"targets\": \"MLflow is an open-source platform for managing \"\n",
    "        \"the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, \"\n",
    "        \"a company that specializes in big data and machine learning solutions. MLflow is \"\n",
    "        \"designed to address the challenges that data scientists and machine learning \"\n",
    "        \"engineers face when developing, training, and deploying machine learning models.\"\n",
    "    },\n",
    ")\n",
    "\n",
    "answer_similarity_metric = answer_similarity(examples=[example])\n",
    "\n",
    "eval_df = pd.DataFrame(\n",
    "    {\n",
    "        \"inputs\": [\n",
    "            \"What is MLflow?\",\n",
    "            \"What is Spark?\",\n",
    "            \"What is Python?\",\n",
    "        ],\n",
    "        \"ground_truth\": [\n",
    "            \"MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is designed to address the challenges that data scientists and machine learning engineers face when developing, training, and deploying machine learning models.\",\n",
    "            \"Apache Spark is an open-source, distributed computing system designed for big data processing and analytics. It was developed in response to limitations of the Hadoop MapReduce computing model, offering improvements in speed and ease of use. Spark provides libraries for various tasks such as data ingestion, processing, and analysis through its components like Spark SQL for structured data, Spark Streaming for real-time data processing, and MLlib for machine learning tasks\",\n",
    "            \"Python is a high-level programming language that was created by Guido van Rossum and released in 1991. It emphasizes code readability and allows developers to express concepts in fewer lines of code than languages like C++ or Java. Python is used in various domains, including web development, scientific computing, data analysis, and machine learning.\",\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    system_prompt = \"Answer the following question in two sentences\"\n",
    "    logged_model = mlflow.openai.log_model(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        task=openai.ChatCompletion,\n",
    "        artifact_path=\"model\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": \"{question}\"},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    results = mlflow.evaluate(\n",
    "        logged_model.model_uri,\n",
    "        eval_df,\n",
    "        evaluators=\"default\",\n",
    "        targets=\"ground_truth\",\n",
    "        model_type=\"question-answering\",\n",
    "        extra_metrics=[\n",
    "            answer_similarity_metric,\n",
    "            make_metric(\n",
    "                eval_fn=custom_metric,\n",
    "                greater_is_better=False,\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "    print(results)\n",
    "\n",
    "    eval_table = results.tables[\"eval_results_table\"]\n",
    "    print(eval_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow-dev-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
