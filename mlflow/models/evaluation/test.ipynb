{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: mlflow[databricks]\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install mlflow[databricks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/04/11 17:56:01 WARNING mlflow.models.evaluation.base: Multiple registered evaluators are found ['default', 'dummy_evaluator'] and they will all be used in evaluation if they support the specified model type. If you want to evaluate with one evaluator, specify the `evaluator` argument and optionally specify the `evaluator_config` argument.\n",
      "2024/04/11 17:56:01 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2024/04/11 17:56:01 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n"
     ]
    },
    {
     "ename": "MlflowException",
     "evalue": "No plugin found for managing model deployments to \"databricks\". In order to deploy models to \"databricks\", find and install an appropriate plugin from https://mlflow.org/docs/latest/plugins.html#community-plugins using your package manager (pip, conda etc).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/mlflow/mlflow/deployments/plugin_manager.py:88\u001b[0m, in \u001b[0;36mDeploymentPlugins.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     87\u001b[0m     target_name \u001b[38;5;241m=\u001b[39m parse_target_uri(item)\n\u001b[0;32m---> 88\u001b[0m     plugin_like \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregistry\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_name\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'databricks'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m\n\u001b[1;32m      8\u001b[0m eval_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m      9\u001b[0m     {\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;66;03m# Input data must be a string column and named \"inputs\".\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     }\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mlflow\u001b[38;5;241m.\u001b[39mstart_run() \u001b[38;5;28;01mas\u001b[39;00m run:\n\u001b[0;32m---> 19\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mendpoints:/databricks-dbrx-instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43minference_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatency\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mlflow/mlflow/models/evaluation/base.py:2103\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, data, model_type, targets, predictions, dataset_path, feature_names, evaluators, evaluator_config, custom_metrics, extra_metrics, custom_artifacts, validation_thresholds, baseline_model, env_manager, model_config, baseline_config, inference_params)\u001b[0m\n\u001b[1;32m   2100\u001b[0m predictions_expected_in_model_output \u001b[38;5;241m=\u001b[39m predictions \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2103\u001b[0m     evaluate_result \u001b[38;5;241m=\u001b[39m \u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluator_name_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluator_name_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluator_name_to_conf_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluator_name_to_conf_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_artifacts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_artifacts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbaseline_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaseline_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictions_expected_in_model_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2115\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2116\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, _ServedPyFuncModel):\n",
      "File \u001b[0;32m~/mlflow/mlflow/models/evaluation/base.py:1253\u001b[0m, in \u001b[0;36m_evaluate\u001b[0;34m(model, model_type, dataset, run_id, evaluator_name_list, evaluator_name_to_conf_map, custom_metrics, extra_metrics, custom_artifacts, baseline_model, predictions)\u001b[0m\n\u001b[1;32m   1251\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m evaluator\u001b[38;5;241m.\u001b[39mcan_evaluate(model_type\u001b[38;5;241m=\u001b[39mmodel_type, evaluator_config\u001b[38;5;241m=\u001b[39mconfig):\n\u001b[1;32m   1252\u001b[0m         _logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating the model with the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevaluator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m evaluator.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1253\u001b[0m         eval_result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1255\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1257\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1258\u001b[0m \u001b[43m            \u001b[49m\u001b[43mevaluator_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1259\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcustom_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1260\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcustom_artifacts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_artifacts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1262\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbaseline_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaseline_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m         eval_results\u001b[38;5;241m.\u001b[39mappend(eval_result)\n\u001b[1;32m   1267\u001b[0m _last_failed_evaluator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mlflow/mlflow/models/evaluation/default_evaluator.py:1962\u001b[0m, in \u001b[0;36mDefaultEvaluator.evaluate\u001b[0;34m(self, model_type, dataset, run_id, evaluator_config, model, custom_metrics, extra_metrics, custom_artifacts, baseline_model, predictions, **kwargs)\u001b[0m\n\u001b[1;32m   1960\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m baseline_model:\n\u001b[1;32m   1961\u001b[0m         _logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating candidate model:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1962\u001b[0m     evaluation_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_baseline_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m baseline_model:\n\u001b[1;32m   1965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m evaluation_result\n",
      "File \u001b[0;32m~/mlflow/mlflow/models/evaluation/default_evaluator.py:1844\u001b[0m, in \u001b[0;36mDefaultEvaluator._evaluate\u001b[0;34m(self, model, is_baseline_model, **kwargs)\u001b[0m\n\u001b[1;32m   1842\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_metrics\u001b[38;5;241m.\u001b[39mremove(extra_metric)\n\u001b[1;32m   1843\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1844\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_model_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_latency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_latency\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_builtin_metrics_by_model_type()\n\u001b[1;32m   1847\u001b[0m eval_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_pred)})\n",
      "File \u001b[0;32m~/mlflow/mlflow/models/evaluation/default_evaluator.py:1406\u001b[0m, in \u001b[0;36mDefaultEvaluator._generate_model_predictions\u001b[0;34m(self, compute_latency)\u001b[0m\n\u001b[1;32m   1403\u001b[0m _logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing model predictions.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute_latency:\n\u001b[0;32m-> 1406\u001b[0m     model_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_with_latency\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_copy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1408\u001b[0m     model_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(X_copy)\n",
      "File \u001b[0;32m~/mlflow/mlflow/models/evaluation/default_evaluator.py:1376\u001b[0m, in \u001b[0;36mDefaultEvaluator._generate_model_predictions.<locals>.predict_with_latency\u001b[0;34m(X_copy)\u001b[0m\n\u001b[1;32m   1374\u001b[0m single_input \u001b[38;5;241m=\u001b[39m row_data\u001b[38;5;241m.\u001b[39mto_frame()\u001b[38;5;241m.\u001b[39mT \u001b[38;5;28;01mif\u001b[39;00m is_dataframe \u001b[38;5;28;01melse\u001b[39;00m row_data\n\u001b[1;32m   1375\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m-> 1376\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msingle_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1377\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   1378\u001b[0m pred_latencies\u001b[38;5;241m.\u001b[39mappend(end_time \u001b[38;5;241m-\u001b[39m start_time)\n",
      "File \u001b[0;32m~/mlflow/mlflow/pyfunc/model.py:547\u001b[0m, in \u001b[0;36m_PythonModelPyfuncWrapper.predict\u001b[0;34m(self, model_input, params)\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_model\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(model_input), params\u001b[38;5;241m=\u001b[39mparams\n\u001b[1;32m    545\u001b[0m     )\n\u001b[1;32m    546\u001b[0m _log_warning_if_params_not_in_predict_signature(_logger, params)\n\u001b[0;32m--> 547\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mlflow/mlflow/models/evaluation/base.py:1338\u001b[0m, in \u001b[0;36m_get_model_from_deployment_endpoint_uri.<locals>.ModelFromDeploymentEndpoint.predict\u001b[0;34m(self, context, model_input)\u001b[0m\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m model_input[input_column]:\n\u001b[1;32m   1336\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1337\u001b[0m         \u001b[38;5;66;03m# If the input data is a string, we will construct the request payload from it.\u001b[39;00m\n\u001b[0;32m-> 1338\u001b[0m         prediction \u001b[38;5;241m=\u001b[39m \u001b[43m_call_deployments_api\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1339\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m   1340\u001b[0m         \u001b[38;5;66;03m# If the input data is a dictionary, we will directly use it as the request\u001b[39;00m\n\u001b[1;32m   1341\u001b[0m         \u001b[38;5;66;03m# payload, with adding the inference parameters if provided.\u001b[39;00m\n\u001b[1;32m   1342\u001b[0m         prediction \u001b[38;5;241m=\u001b[39m _call_deployments_api(\n\u001b[1;32m   1343\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint, data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams, wrap_payload\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1344\u001b[0m         )\n",
      "File \u001b[0;32m~/mlflow/mlflow/metrics/genai/model_utils.py:134\u001b[0m, in \u001b[0;36m_call_deployments_api\u001b[0;34m(deployment_uri, payload, eval_parameters, wrap_payload)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeployments\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_deploy_client\n\u001b[0;32m--> 134\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mget_deploy_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m endpoint \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mget_endpoint(deployment_uri)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# TODO: Standardize the return type of `get_endpoint` and remove this check\u001b[39;00m\n",
      "File \u001b[0;32m~/mlflow/mlflow/deployments/interface.py:60\u001b[0m, in \u001b[0;36mget_deploy_client\u001b[0;34m(target_uri)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     59\u001b[0m target \u001b[38;5;241m=\u001b[39m parse_target_uri(target_uri)\n\u001b[0;32m---> 60\u001b[0m plugin \u001b[38;5;241m=\u001b[39m \u001b[43mplugin_store\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, obj \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39mgetmembers(plugin):\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misclass(obj):\n",
      "File \u001b[0;32m~/mlflow/mlflow/deployments/plugin_manager.py:97\u001b[0m, in \u001b[0;36mDeploymentPlugins.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo plugin found for managing model deployments to \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIn order to deploy models to \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, find and install an appropriate \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour package manager (pip, conda etc).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     96\u001b[0m     )\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(msg, error_code\u001b[38;5;241m=\u001b[39mRESOURCE_DOES_NOT_EXIST)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(plugin_like, entrypoints\u001b[38;5;241m.\u001b[39mEntryPoint):\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mMlflowException\u001b[0m: No plugin found for managing model deployments to \"databricks\". In order to deploy models to \"databricks\", find and install an appropriate plugin from https://mlflow.org/docs/latest/plugins.html#community-plugins using your package manager (pip, conda etc)."
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.deployments import set_deployments_target\n",
    "import pandas as pd\n",
    "\n",
    "# Point the client to Databricks Foundation Models API\n",
    "set_deployments_target(\"databricks\")\n",
    "\n",
    "eval_data = pd.DataFrame(\n",
    "    {\n",
    "        # Input data must be a string column and named \"inputs\".\n",
    "        \"inputs\": [\n",
    "            \"Write 3 reasons why you should use MLflow?\",\n",
    "            \"Can you explain the difference between classification and regression?\",\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    results = mlflow.evaluate(\n",
    "        model=\"endpoints:/databricks-dbrx-instruct\",\n",
    "        data=eval_data,\n",
    "        inference_params={\"max_tokens\": 100, \"temperature\": 0.0},\n",
    "        model_type=\"text\",\n",
    "        extra_metrics=[mlflow.metrics.latency()]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow-dev-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
